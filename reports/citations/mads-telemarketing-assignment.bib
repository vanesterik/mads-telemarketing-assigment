@online{AdaBoostClassifier,
  title = {{{AdaBoostClassifier}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Classifier comparison Multi-class AdaBoosted Decision Trees Two-class AdaBoost Plot the decision surfaces of ensembles of trees on the iris dataset},
  langid = {english},
  organization = {scikit-learn}
}

@inreference{CategoricalVariable2025,
  title = {Categorical Variable},
  booktitle = {Wikipedia},
  date = {2025-01-31T04:38:11Z},
  url = {https://en.wikipedia.org/w/index.php?title=Categorical_variable&oldid=1272986862},
  urldate = {2025-06-11},
  abstract = {In statistics, a categorical variable (also called qualitative variable) is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. In computer science and some branches of mathematics, categorical variables are referred to as enumerations or enumerated types. Commonly (though not in this article), each of the possible values of a categorical variable is referred to as a level. The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from observations made of qualitative data that are summarised as counts or cross tabulations, or from observations of quantitative data grouped within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term "categorical data" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables. Ordinal variables have a meaningful ordering, while nominal variables have no meaningful ordering. A categorical variable that can take on exactly two values is termed a binary variable or a dichotomous variable; an important special case is the Bernoulli variable. Categorical variables with more than two possible values are called polytomous variables; categorical variables are often assumed to be polytomous unless otherwise specified. Discretization is treating continuous data as if it were categorical. Dichotomization is treating continuous data or polytomous variables as if they were binary variables. Regression analysis often treats category membership with one or more quantitative dummy variables.},
  langid = {english},
  annotation = {Page Version ID: 1272986862},
  file = {/Users/koendirkvanesterik/Zotero/storage/YIAPIFW5/Categorical_variable.html}
}

@online{DataScienceBusiness,
  title = {Data {{Science}} for {{Business}}: {{What}} You Need to Know about {{Data Mining}} and {{Data Analytic Thinking}}},
  shorttitle = {Data {{Science}} for {{Business}}},
  url = {https://fosterprovost.com/publication/data-science-for-business-what-you-need-to-know-about-data-mining-and-data-analytic-thinking/},
  urldate = {2025-07-05},
  abstract = {Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the” data-analytic thinking” necessary for extracting useful knowledge and business value from the data you collect.~ This guide also helps you understand the many data mining techniques in use […]},
  langid = {american},
  organization = {Foster Provost},
  file = {/Users/koendirkvanesterik/Zotero/storage/27WCE9AI/data-science-for-business-what-you-need-to-know-about-data-mining-and-data-analytic-thinking.html}
}

@software{esterikVanesterikMadstelemarketingassignment2025,
  title = {Vanesterik/Mads-Telemarketing-Assignment},
  author = {family=Esterik, given=Koen, prefix=van, useprefix=false},
  date = {2025-07-04T18:02:56Z},
  origdate = {2025-05-28T11:14:17Z},
  url = {https://github.com/vanesterik/mads-telemarketing-assignment},
  urldate = {2025-07-05},
  abstract = {Telemarketing Assignment / Predictive Modelling (2024 P3A) / MADS / HAN}
}

@inreference{EvaluationBinaryClassifiers2025,
  title = {Evaluation of Binary Classifiers},
  booktitle = {Wikipedia},
  date = {2025-05-25T16:56:15Z},
  url = {https://en.wikipedia.org/w/index.php?title=Evaluation_of_binary_classifiers&oldid=1292184015},
  urldate = {2025-07-06},
  abstract = {Evaluation of a binary classifier typically assigns a numerical value, or values, to a classifier that represent its accuracy. An example is error rate, which measures how frequently the classifier makes a mistake. There are many metrics that can be used; different fields have different preferences. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred.  An important distinction is between metrics that are independent of the prevalence or skew (how often each class occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties. Often, evaluation is used to compare two methods of classification, so that one can be adopted and the other discarded. Such comparisons are more directly achieved by a form of evaluation that results in a single unitary metric rather than a pair of metrics.},
  langid = {english},
  annotation = {Page Version ID: 1292184015}
}

@online{GradientBoostingClassifier,
  title = {{{GradientBoostingClassifier}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Feature transformations with ensembles of trees Gradient Boosting Out-of-Bag estimates Gradient Boosting regularization Feature discretization},
  langid = {english},
  organization = {scikit-learn}
}

@online{KNeighborsClassifier,
  title = {{{KNeighborsClassifier}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Classifier comparison Caching nearest neighbors Nearest Neighbors Classification Comparing Nearest Neighbors with and without Neighborhood Components Analysis Dimensionality Reduc...},
  langid = {english},
  organization = {scikit-learn}
}

@online{LabelEncoder,
  title = {{{LabelEncoder}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html},
  urldate = {2025-07-06},
  langid = {english},
  organization = {scikit-learn},
  file = {/Users/koendirkvanesterik/Zotero/storage/6QCP4UXN/sklearn.preprocessing.LabelEncoder.html}
}

@online{LogisticRegression,
  title = {{{LogisticRegression}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.linear_model.LogisticRegression.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Probability Calibration curves Plot classification probability Column Transformer with Mixed Types Pipelining: chaining a PCA and a logistic regression Feature transformations wit...},
  langid = {english},
  organization = {scikit-learn}
}

@online{MinMaxScaler,
  title = {{{MinMaxScaler}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Time-related feature engineering Image denoising using kernel PCA Selecting dimensionality reduction with Pipeline and GridSearchCV Univariate Feature Selection Recursive feature ...},
  langid = {english},
  organization = {scikit-learn},
  file = {/Users/koendirkvanesterik/Zotero/storage/ZH87Z8IL/sklearn.preprocessing.MinMaxScaler.html}
}

@article{moroDatadrivenApproachPredict2014,
  title = {A Data-Driven Approach to Predict the Success of Bank Telemarketing},
  author = {Moro, Sérgio and Cortez, Paulo and Rita, Paulo},
  date = {2014-06-01},
  journaltitle = {Decision Support Systems},
  shortjournal = {Decision Support Systems},
  volume = {62},
  pages = {22--31},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2014.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S016792361400061X},
  urldate = {2025-06-11},
  abstract = {We propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits. A Portuguese retail bank was addressed, with data collected from 2008 to 2013, thus including the effects of the recent financial crisis. We analyzed a large set of 150 features related with bank client, product and social-economic attributes. A semi-automatic feature selection was explored in the modeling phase, performed with the data prior to July 2012 and that allowed to select a reduced set of 22 features. We also compared four DM models: logistic regression, decision trees (DTs), neural network (NN) and support vector machine. Using two metrics, area of the receiver operating characteristic curve (AUC) and area of the LIFT cumulative curve (ALIFT), the four models were tested on an evaluation set, using the most recent data (after July 2012) and a rolling window scheme. The NN presented the best results (AUC=0.8 and ALIFT=0.7), allowing to reach 79\% of the subscribers by selecting the half better classified clients. Also, two knowledge extraction methods, a sensitivity analysis and a DT, were applied to the NN model and revealed several key attributes (e.g., Euribor rate, direction of the call and bank agent experience). Such knowledge extraction confirmed the obtained model as credible and valuable for telemarketing campaign managers.},
  keywords = {Bank deposits,Classification,Neural networks,Savings,Telemarketing,Variable selection},
  file = {/Users/koendirkvanesterik/Zotero/storage/9W5YAD9K/Moro et al. - 2014 - A data-driven approach to predict the success of b.pdf;/Users/koendirkvanesterik/Zotero/storage/DRDP79LN/S016792361400061X.html}
}

@online{OneHotEncoder,
  title = {{{OneHotEncoder}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Time-related feature engineering Column Transformer with Mixed Types Feature transformations with ensembles of trees Categorical Feature Support in Gradient Boosting Combine predi...},
  langid = {english},
  organization = {scikit-learn},
  file = {/Users/koendirkvanesterik/Zotero/storage/I7YW7VLP/sklearn.preprocessing.OneHotEncoder.html}
}

@inreference{PrinciplesCorporateFinance2024,
  title = {\emph{Principles of }{{\emph{Corporate Finance}}}},
  booktitle = {Wikipedia},
  date = {2024-02-04T16:58:05Z},
  url = {https://en.wikipedia.org/w/index.php?title=Principles_of_Corporate_Finance&oldid=1203323000},
  urldate = {2025-06-23},
  abstract = {Principles of Corporate Finance is a reference work on the corporate finance theory edited by Richard Brealey, Stewart Myers, Franklin Allen, and Alex Edmans. The book is one of the leading texts that describes the theory and practice of corporate finance. It was initially published in October 1980 and now is available in its 14th edition. Principles of Corporate Finance has earned loyalty both as a classroom tool and as a professional reference book.},
  langid = {english},
  annotation = {Page Version ID: 1203323000},
  file = {/Users/koendirkvanesterik/Zotero/storage/CGWIW3W5/Principles_of_Corporate_Finance.html}
}

@online{RandomForestClassifier,
  title = {{{RandomForestClassifier}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Probability Calibration for 3-class classification Comparison of Calibration of Classifiers Classifier comparison Inductive Clustering OOB Errors for Random Forests Feature transf...},
  langid = {english},
  organization = {scikit-learn}
}

@dataset{s.moroBankMarketing2014,
  title = {Bank {{Marketing}}},
  author = {S. Moro, P. Rita},
  date = {2014},
  publisher = {UCI Machine Learning Repository},
  doi = {10.24432/C5K306},
  url = {https://archive.ics.uci.edu/dataset/222},
  urldate = {2025-07-06}
}

@online{Train_test_split,
  title = {Train\_test\_split},
  url = {https://scikit-learn/stable/modules/generated/sklearn.model_selection.train_test_split.html},
  urldate = {2025-07-06},
  abstract = {Gallery examples: Image denoising using kernel PCA Faces recognition example using eigenfaces and SVMs Model Complexity Influence Prediction Latency Lagged features for time series forecasting Prob...},
  langid = {english},
  organization = {scikit-learn}
}

@online{XGBClassifier10:23:00+00:00,
  title = {{{XGBClassifier}}},
  year = {10:23:00+00:00},
  url = {https://www.geeksforgeeks.org/machine-learning/xgbclassifier/},
  urldate = {2025-07-06},
  abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
  langid = {american},
  organization = {GeeksforGeeks}
}

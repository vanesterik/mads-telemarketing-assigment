{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e4f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for the project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline as base_make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "import calendar\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63bb779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the telemarketing project\n",
    "from pathlib import Path\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"contact\",\n",
    "    \"day_of_week\",\n",
    "    \"default\",\n",
    "    \"education\",\n",
    "    \"housing\",\n",
    "    \"job\",\n",
    "    \"loan\",\n",
    "    \"marital\",\n",
    "    \"month\",\n",
    "    \"poutcome\",\n",
    "    \"year\",\n",
    "]\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"age\",\n",
    "    \"campaign\",\n",
    "    \"pdays\",\n",
    "    \"previous\",\n",
    "    \"emp.var.rate\",\n",
    "    \"cons.price.idx\",\n",
    "    \"cons.conf.idx\",\n",
    "    \"euribor3m\",\n",
    "    \"nr.employed\",\n",
    "]\n",
    "BINARY_FEATURES = [\n",
    "    \"y\",\n",
    "]\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "INTERIM_DATA_DIR = DATA_DIR / \"interim\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "DATA_FILENAME = \"bank-additional-full.csv\"\n",
    "APPROACHED_DATA_FILENAME = \"approached_data.csv\"\n",
    "NOT_APPROACHED_DATA_FILENAME = \"not_approached_data.csv\"\n",
    "\n",
    "HONOLULU_BLUE = \"#1F77B4\"\n",
    "IMPERIAL_RED = \"#F0534F\"\n",
    "PERSIAN_GREEN = \"#27A69A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55eaff",
   "metadata": {},
   "source": [
    "### Time-based Data Split Strategy\n",
    "\n",
    "- **Test period selection**: 2010 data serves as the test set to ensure realistic and future-proof evaluation\n",
    "\n",
    "- **Training and validation periods**: 2008-2009 data is used for model training and validation phases\n",
    "\n",
    "- **Temporal integrity**: The time-based split of training and test datasets maintains chronological order, preventing data leakage where future information inappropriately influences model training\n",
    "\n",
    "- **Generalization assessment**: This approach enables honest evaluation of how effectively the model performs on completely unseen data\n",
    "\n",
    "- **Business application**: The split structure supports realistic business strategy simulations that mirror actual real-world deployment conditions\n",
    "\n",
    "**note**\n",
    "\n",
    "We deliberately avoided stratifying the data splits by the target variable. Stratification requires random shuffling, which is fundamentally incompatible with the strict chronological ordering necessary for a time-series problem.\n",
    "Our priority is to prevent data leakage and create a realistic train/test split that respects the arrow of time. The class imbalance is instead addressed at the model training stage using the class_weight parameter. This method correctly handles the imbalance without compromising the temporal integrity of our validation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99850f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/not_approached_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load not_approached dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROCESSED_DATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mNOT_APPROACHED_DATA_FILENAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mads/mads-telemarketing-assignment/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mads/mads-telemarketing-assignment/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mads/mads-telemarketing-assignment/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mads/mads-telemarketing-assignment/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/mads/mads-telemarketing-assignment/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/processed/not_approached_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load not_approached dataset\n",
    "df = pd.read_csv(PROCESSED_DATA_DIR / NOT_APPROACHED_DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y for modeling\n",
    "X = df.drop(columns=[\"y\"], axis=1)\n",
    "y = df[\"y\"]\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dertemine the split in train and test data.\n",
    "print(X.groupby(['year']).size().sort_index())\n",
    "# Ratio between 2010 data and the rest\n",
    "print(f\"Ratio of 2010 data to the rest: {X[X['year'] == 2010].shape[0] / X[X['year'] != 2010].shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4067260",
   "metadata": {},
   "source": [
    "Our time-based data split revealed a key challenge: the most recent data available for testing (2010) is significantly smaller than the preceding training years. Standard evaluation on this set alone would be statistically unreliable.\n",
    "\n",
    "Therefore, we have adopted a more sophisticated validation protocol to ensure trustworthiness:\n",
    "\n",
    "### Model selection will be driven by a stable, averaged score from a rigorous time-series cross-validation on the large 2008-2009 dataset\n",
    "\n",
    "**The Problem with unequal spaced time series:**\n",
    "\n",
    "TimeSeriesSplit splits by row count, not actual time duration. This means:\n",
    "1. **Unequal Spacing:** Test sets (same number of rows) can cover vastly different **time durations**.\n",
    "2. **Imbalanced Periods:** Different test sets will represent different years/contexts (e.g., 2008 data vs. 2009 data).\n",
    "\n",
    "**Result:** Cross-validation metrics (RMSE, MAE) are **not truly comparable** across folds, as they reflect performance over different time horizons and contexts.\n",
    "\n",
    "**How to Address It:**\n",
    "1. **Keep TimeSeriesSplit**: It's still correct for chronological splits.\n",
    "2. **Smart Feature Engineering (Key!)**:\n",
    "   * **Time-based Features:** Add year, month, day_of_week, etc., so the model knows the temporal context.\n",
    "   * **Gap Features:** Add days_since_last_observation or observations_in_last_X_days to inform the model about data density.\n",
    "3. **Careful Evaluation**:\n",
    "   * Report **metrics for each individual fold** to see how performance shifts over time/contexts.\n",
    "   * **Contextualize** results; explain performance changes based on the different periods covered by each fold\n",
    "\n",
    "\n",
    "### The small 2010 test set will then serve as a final, mandatory sanity check, with its performance explicitly framed by confidence intervals to account for potential variance.\n",
    "\n",
    "**References need to be checked**\n",
    "\n",
    "Statistical Issues with Small Test Sets\n",
    "This issue is well-documented in statistics and machine learning:\n",
    "\n",
    "Reference: Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.\n",
    "Chapter 7 discusses model assessment and the impact of sample size on estimation stability. Small test sets lead to high-variance estimates, necessitating robust evaluation methods like cross-validation or confidence intervals.\n",
    "\n",
    "\n",
    "Reference: Kohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. IJCAI.\n",
    "Kohavi emphasizes that small test sets produce unreliable performance estimates and recommends techniques like cross-validation or confidence intervals to quantify uncertainty.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870299a",
   "metadata": {},
   "source": [
    "### Concept drift and temporal imbalance\n",
    "\n",
    "Our dataset is influenced by two interacting temporal phenomena. First, concept drift, which could be driven by for example the 2008 financial crisis, has rendered older data less predictive of current outcomes. Second, we observe a temporal instance imbalance, where this outdated 2008-era data is overrepresented in volume. While each issue is problematic on its own, their combination is especially harmful: the volume imbalance significantly amplifies the negative impact of the drift. To address this, we apply time-based sample weighting — not to equalize instance counts, but to rebalance the influence of different time periods. This ensures the model prioritizes learning from more recent, and therefore more relevant, observations.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92c4ec",
   "metadata": {},
   "source": [
    "### Feature transformation \n",
    "\n",
    "Some distributions have a heavy tail. In order to quickly check this the describe function can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929270cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading not approached dataset\n",
    "not_approached = pd.read_csv(PROCESSED_DATA_DIR / NOT_APPROACHED_DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decribe the dataset\n",
    "not_approached.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1578be0",
   "metadata": {},
   "source": [
    "Age: Is slightly skewed and can be handled with standard scaler.\n",
    "\n",
    "Campaign: Is serverly skewed and problamatic for standardization. Max value is 56 while 75%Q is 3 and the mean is 2.567. Apply power transformation like log.\n",
    "\n",
    "emp.var.rate: Does not have any signs of skew\n",
    "\n",
    "cons.price.idx: Does not have any signs of skew\n",
    "\n",
    "cons.conf.idx: Does not have any signs of skew\n",
    "\n",
    "euribor3m: Does not have any signs of skew\n",
    "\n",
    "nr.empployed: Does not have any signs of skew\n",
    "\n",
    "year: Does not have any signs of skew\n",
    "\n",
    "y: target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load approached dataset\n",
    "approached = pd.read_csv(PROCESSED_DATA_DIR / APPROACHED_DATA_FILENAME)\n",
    "approached.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a013e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decribe features pdays and previous\n",
    "approached.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7381d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create distribution plots for pdays and previous\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "approached[\"pdays\"].hist(bins=50, ax=axes[0], color=HONOLULU_BLUE)\n",
    "axes[0].set_title(\"Distribution of Pdays\")\n",
    "axes[0].set_xlabel(\"Pdays\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "approached[\"previous\"].hist(bins=50, ax=axes[1], color=HONOLULU_BLUE)\n",
    "axes[1].set_title(\"Distribution of Previous\")\n",
    "axes[1].set_xlabel(\"Previous\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b669b03",
   "metadata": {},
   "source": [
    "** Analysis of appoached values\n",
    "\n",
    "Age: Is slightly skewed and can be handled with standard scaler.\n",
    "\n",
    "Campaign: Is serverly skewed and problamatic for standardization. Max value is 13 while 75%Q is 2 and the mean is 1.82. Apply power transformation like log.\n",
    "\n",
    "pdays: Moderate skew, log transformation advised\n",
    "\n",
    "previous: Strong skew, log transformation advised\n",
    "\n",
    "emp.var.rate: Does not have any signs of skew\n",
    "\n",
    "cons.price.idx: Does not have any signs of skew\n",
    "\n",
    "cons.conf.idx: Does not have any signs of skew\n",
    "\n",
    "euribor3m: Does not have any signs of skew\n",
    "\n",
    "nr.empployed: Does not have any signs of skew\n",
    "\n",
    "year: Does not have any signs of skew\n",
    "\n",
    "y: target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6411971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation to pdays and plot the distribution\n",
    "approached[\"log_pdays\"] = np.log1p(approached[\"pdays\"])\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "approached[\"log_pdays\"].hist(bins=50, ax=ax, color=HONOLULU_BLUE)\n",
    "ax.set_title(\"Log Transformation of Pdays\") \n",
    "ax.set_xlabel(\"Log(Pdays)\")\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation to previous and plot the distribution\n",
    "approached[\"log_previous\"] = np.log1p(approached[\"previous\"])\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "approached[\"log_previous\"].hist(bins=50, ax=ax, color=HONOLULU_BLUE)\n",
    "ax.set_title(\"Log Transformation of Previous\")\n",
    "ax.set_xlabel(\"Log(Previous)\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the log transformed features\n",
    "approached[[\"log_pdays\", \"log_previous\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d281bd8",
   "metadata": {},
   "source": [
    "The log transformation of previous did not normalize the data. However it was a good step in the right direction since extreme values are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59189bb0",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "**Core metric**\n",
    "\n",
    "The project aims to create stakeholder value by maximizing the profit of the telemarketing department. Profit maximalization based ont graph??\n",
    "\n",
    "- Expected Profit = (Number of True Positives * Revenue_per_sale) - (Number of Calls Made * Cost_per_call)\n",
    "\n",
    "additionally\n",
    "\n",
    "\n",
    "\n",
    "**Model calibration**\n",
    "\n",
    "Calabritaion is essential since the strategic eval module depends on probabilities. Why needed:\n",
    "- Meaningfull thresholds: if the probabilities are not calibrated there is no meaning in 'call all customers with  >0.5 probability. If the model is overconfident, the actual chance of success might be way lower.\n",
    "\n",
    "- Accurrate profit predictions: The profit simulations relies on the predicted probabilities. Incorrect probabilities will result in incorrect profit estimations.\n",
    "\n",
    "- Trust: For the stakeholder to make a good data driven decision they need to trust the numbers the model generates.\n",
    "\n",
    "How to assess calibration (to do)\n",
    "\n",
    "- Reliability diagram (Calibration curve)\n",
    "- Brier score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae5ae6",
   "metadata": {},
   "source": [
    "Notities:\n",
    "\n",
    "Wat is de winst op basis van de confusion matrix. Kosten en baten.\n",
    "\n",
    "Het beste model is het model dat de beste winst realiseerd. Train beste model.\n",
    "\n",
    "Winst per gebelde persoon, om test en train set goed te kunnen vergelijken.\n",
    "\n",
    "Voor stakeholders, hoofdzaken van het onderzoek. Dus niet te technisch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8cf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean of calls during this campaign for the not approached dataset\n",
    "mean_calls = not_approached[\"campaign\"].mean() \n",
    "print(f\"Mean calls during this campaign for not approached dataset: {mean_calls:.2f} times\")\n",
    "# the average duration of calls for the not approached dataset\n",
    "# needs to be claculated for the not approached set but column is dropped\n",
    "# quick peak in the original dataset leads to a mean of 258 seconds\n",
    "mean_duration_minutes = 258 / 60\n",
    "print(f\"Mean duration of calls for not approached dataset: {mean_duration_minutes:.2f} minutes\")\n",
    "# average time spent per customer\n",
    "average_time_on_phone_per_customer = mean_duration_minutes * mean_calls\n",
    "print(f\"Average time spent per customer on phone: {average_time_on_phone_per_customer:.2f} minutes\")\n",
    "# including the start and end of the call including the time to prepare and follow up\n",
    "# preparation and follow up is estimated to be 3 minutes per call\n",
    "prep_time_per_call = 3\n",
    "total_prep_time = prep_time_per_call * mean_calls\n",
    "average_time_per_customer = average_time_on_phone_per_customer + total_prep_time\n",
    "print(f\"Average time spent per customer including preparation and follow up: {average_time_per_customer:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the current profit for the not approached dataset for a selected year\n",
    "selected_year = 2010\n",
    "hourly_wage = 35  # in euro\n",
    "\n",
    "subset = not_approached[not_approached[\"year\"] == selected_year]\n",
    "\n",
    "cost_per_contact = hourly_wage * (average_time_per_customer / 60)\n",
    "profit_per_success = 200\n",
    "\n",
    "total_costs = subset.shape[0] * cost_per_contact\n",
    "profit = subset[subset[\"y\"] == 1].shape[0] * profit_per_success\n",
    "\n",
    "total_profit = profit - total_costs\n",
    "print(f\"Cost per contact: {cost_per_contact:.2f} euro\")\n",
    "formatted_profit = f\"{total_profit:,.0f}\".replace(\",\", \".\")\n",
    "print(f\"Total profit for year {selected_year}: {formatted_profit} euro\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads-telemarketing-assignment-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
